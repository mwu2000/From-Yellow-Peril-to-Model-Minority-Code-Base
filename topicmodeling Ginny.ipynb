{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f5851d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-af871b02aa86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbertweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vinai/bertweet-covid19-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vinai/bertweet-covid19-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/REU_2021/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/REU_2021/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \"\"\"\n\u001b[1;32m    445\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/REU_2021/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             resolved_config_file = cached_path(\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/REU_2021/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/REU_2021/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/REU_2021/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \u001b[0mcontent_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m     progress = tqdm(\n\u001b[0m\u001b[1;32m   1444\u001b[0m         \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/REU_2021/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/REU_2021/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-covid19-base-uncased\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-covid19-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b128b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner2(x):\n",
    "    import re\n",
    "    x = re.sub(r'https:\\/\\/.*[\\r\\n]*', '', x)\n",
    "    x = re.sub(',', '',x)\n",
    "    x = re.sub('\\?','',x)\n",
    "    x = re.sub('!','',x)\n",
    "    x = re.sub('\\n', ' ',x)\n",
    "    x = re.sub(\"@[A-Za-z0-9]+\",'', x)\n",
    "    x = x.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    \n",
    "    #from gensim.parsing.preprocessing import remove_stopwords\n",
    "    #x = remove_stopwords(x)\n",
    "    \n",
    "    import emoji\n",
    "    cleaned = emoji.get_emoji_regexp().sub(\"\", x)\n",
    "\n",
    "    return cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/wangxinyu/Downloads/Hate-Narratives-main/hydrated.csv\"\n",
    "colnames = ['tweet_id', 'text', 'num_likes', 'date', 'author']\n",
    "hydrated_data = pd.read_csv(file_path, names = colnames)\n",
    "hydrated_data.info()\n",
    "hydrated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bertcorpus=hydrated_data.apply(lambda x: cleaner2(x['text']), axis=1)\n",
    "clean_bertcorpus2=clean_bertcorpus.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ea336",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_bertcorpus2))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids=hydrated_data['tweet_id'].tolist()\n",
    "text=hydrated_data['text'].tolist()\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d663a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features=[]\n",
    "for item in clean_bertcorpus2:\n",
    "    print(item)\n",
    "    input_ids = torch.tensor([tokenizer.encode(item)])\n",
    "    print(np.shape(input_ids))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature=bertweet(input_ids).last_hidden_state[0,0,:].tolist()\n",
    "        print(np.shape(feature))\n",
    "        features.append(feature)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730960fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c848f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acef64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "\n",
    "additional_stop_words = [\"ur\", '21', 'amb', 'blah', 'till', 'yep', 'rt', 'yall', '23', '19', 'guy', 'hes', '14',\n",
    "                        'okay', 'whats', 'huh', 'jan', 'did', 'didnt', 'mean', 'does', 'youre', 'yeah', 'hey', 'just',\n",
    "                        'like', 'yes', 'yessss', 'yesssss', 'plz', '40', 'agree', 'zy', 'zvinopfuura', 'yess',\n",
    "                         'zzzzzzzz', 'zzzzzzzz', 'yessssss', 'yesss', 'gt', 'gtgtgt', 'etcwith', 'etchowever', 'etcwhy',\n",
    "                        'etcthere', 'etcthey', 'etcpoc', 'zzzzzz', 'etcit', 'etcthe', 'etcon', 'etcl',\n",
    "                        'fuck', 'etcetera', 'etcetc','im']\n",
    "s_words = text.ENGLISH_STOP_WORDS.union(additional_stop_words)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_custom = CountVectorizer(max_df = 0.70, min_df = 0.05, stop_words = s_words)\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "#HDBSCAN\n",
    "hdbscan_custom = HDBSCAN(min_cluster_size=15, metric='euclidean', \n",
    "                        cluster_selection_method='leaf', prediction_data = True, cluster_selection_epsilon = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e380e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alberto = BERTopic(language='english', \n",
    "                   calculate_probabilities=True, \n",
    "                   verbose=True, \n",
    "                   vectorizer_model=vectorizer_custom,\n",
    "                   hdbscan_model= hdbscan_custom\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161bba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array=np.array(features)\n",
    "features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probabilities = alberto.fit_transform(clean_bertcorpus2, features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = alberto.get_topic_info()\n",
    "freq\n",
    "#alberto.get_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alberto.save('bertopic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124790f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = pd.read_csv(\"all_tweet_ids.csv\")\n",
    "\n",
    "labels = (og_data.loc[(og_data.Label == 'Hate') | (og_data.Label == 'Counterhate')]\\\n",
    "                     .rename(columns = {\"Tweet ID\" : \"tweet_id\"})\n",
    "             )\n",
    "\n",
    "#join label information to hydrated data\n",
    "hydrated_data1 = pd.merge(hydrated_data, labels, how = 'left', on = 'tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e05e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hate=0\n",
    "count_Counterhate=0\n",
    "Else=0\n",
    "hydrated_data2=hydrated_data1[0:1000]\n",
    "for i in hydrated_data2['Label']:\n",
    "    if i=='Counterhate':\n",
    "        count_Counterhate+=1\n",
    "    elif i=='Hate':\n",
    "        count_hate+=1\n",
    "    else:\n",
    "        Else+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_hate,count_Counterhate,Else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f05720",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrated_data1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d0edf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0e93095a560c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m umap_embeddings = umap.UMAP(n_neighbors=15, \n\u001b[1;32m      7\u001b[0m                             \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                             metric='cosine').fit_transform(features_array)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_array' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "colors = itertools.cycle([\"r\", \"b\", \"g\"])\n",
    "import umap\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=768, \n",
    "                            metric='cosine').fit_transform(features_array)\n",
    "import hdbscan\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n",
    "\n",
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(features_array)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "#plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(outliers.x, outliers.y, color=next(colors), s=0.1)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.1, cmap='hsv_r')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70184f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alberto.visualize_distribution(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alberto.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416c0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
